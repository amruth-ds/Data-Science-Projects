{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.1.2.30-cp37-cp37m-win_amd64.whl (33.0 MB)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\amruth pc\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from opencv-python) (1.18.1)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.1.2.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"
     ]
    }
   ],
   "source": [
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C:/Users/Amruth PC/Desktop/SEM 2/AI/data/helmet_detection/training\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialising the CNN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convolution(64 feature detector of dimension 3 by 3), input shape 3 layer for color image)\n",
    "classifier.add(Conv2D(64,(3,3),input_shape = (64,64,3), activation = 'relu'))\n",
    "## MaxPooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "## Add another layer\n",
    "classifier.add(Conv2D(64,(3,3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "## Add another layer\n",
    "classifier.add(Conv2D(64,(3,3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flattening\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fully connected ANN, Hidden ANN and output layer\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output layer\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compliling\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 62, 62, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 29, 29, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 12, 12, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               295040    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 370,817\n",
      "Trainable params: 370,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data importing and transforming and scaling\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling test data\n",
    "##no  data augmentation\n",
    "test_datagen = ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 647 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Importing training data\n",
    "train_set = train_datagen.flow_from_directory('C:/Users/Amruth PC/Desktop/SEM 2/AI/data/helmet_detection/training',\n",
    "                                               target_size=(64, 64),\n",
    "                                               \n",
    "                                               class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'with helmet': 0, 'without helmet': 1}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'with helmet': 0, 'without helmet': 1}\n"
     ]
    }
   ],
   "source": [
    "#which is cat which is dog?\n",
    "label_map = (train_set.class_indices)\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 208 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "## Importng test data\n",
    "test_set = test_datagen.flow_from_directory('C:/Users/Amruth PC/Desktop/SEM 2/AI/data/helmet_detection/testing',\n",
    "                                            target_size=(64, 64),\n",
    "                                            \n",
    "                                            class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.image.DirectoryIterator at 0x21206885048>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "21/21 [==============================] - 3s 130ms/step - loss: 0.0094 - accuracy: 0.9969 - val_loss: 0.0180 - val_accuracy: 0.9808\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - 3s 151ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0246 - val_accuracy: 0.9712\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - 3s 152ms/step - loss: 0.0250 - accuracy: 0.9954 - val_loss: 0.0120 - val_accuracy: 0.9712\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - 3s 132ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0648 - val_accuracy: 0.9952\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - 3s 132ms/step - loss: 0.0049 - accuracy: 0.9985 - val_loss: 7.3106e-04 - val_accuracy: 0.9567\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - 3s 160ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0094 - val_accuracy: 0.9808\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - 4s 167ms/step - loss: 0.0246 - accuracy: 0.9876 - val_loss: 0.0685 - val_accuracy: 0.9808\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - 3s 145ms/step - loss: 0.0308 - accuracy: 0.9861 - val_loss: 3.1929e-04 - val_accuracy: 0.9856\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - 3s 161ms/step - loss: 0.0178 - accuracy: 0.9923 - val_loss: 0.3584 - val_accuracy: 0.9760\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - 3s 148ms/step - loss: 0.0149 - accuracy: 0.9923 - val_loss: 0.2230 - val_accuracy: 0.9760\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - 3s 146ms/step - loss: 0.0094 - accuracy: 0.9969 - val_loss: 0.0027 - val_accuracy: 0.9808\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - 3s 165ms/step - loss: 0.0150 - accuracy: 0.9969 - val_loss: 0.1114 - val_accuracy: 0.9663\n",
      "Epoch 13/100\n",
      "21/21 [==============================] - 3s 142ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0205 - val_accuracy: 0.9615\n",
      "Epoch 14/100\n",
      "21/21 [==============================] - 3s 148ms/step - loss: 0.0265 - accuracy: 0.9923 - val_loss: 0.4787 - val_accuracy: 0.9519\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - 3s 143ms/step - loss: 0.0537 - accuracy: 0.9815 - val_loss: 0.0424 - val_accuracy: 0.9760\n",
      "Epoch 16/100\n",
      "21/21 [==============================] - 3s 148ms/step - loss: 0.1052 - accuracy: 0.9583 - val_loss: 0.5981 - val_accuracy: 0.9712\n",
      "Epoch 17/100\n",
      "21/21 [==============================] - 3s 154ms/step - loss: 0.0321 - accuracy: 0.9892 - val_loss: 0.1126 - val_accuracy: 0.9663\n",
      "Epoch 18/100\n",
      "21/21 [==============================] - 3s 150ms/step - loss: 0.0480 - accuracy: 0.9815 - val_loss: 0.0098 - val_accuracy: 0.9519\n",
      "Epoch 19/100\n",
      "21/21 [==============================] - 3s 148ms/step - loss: 0.0398 - accuracy: 0.9845 - val_loss: 0.0043 - val_accuracy: 0.9663\n",
      "Epoch 20/100\n",
      "21/21 [==============================] - 3s 155ms/step - loss: 0.0423 - accuracy: 0.9861 - val_loss: 0.1154 - val_accuracy: 0.9663\n",
      "Epoch 21/100\n",
      "21/21 [==============================] - 3s 149ms/step - loss: 0.0201 - accuracy: 0.9954 - val_loss: 0.0361 - val_accuracy: 0.9808\n",
      "Epoch 22/100\n",
      "21/21 [==============================] - 3s 164ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0091 - val_accuracy: 0.9808\n",
      "Epoch 23/100\n",
      "21/21 [==============================] - 3s 142ms/step - loss: 0.0114 - accuracy: 0.9969 - val_loss: 7.7703e-04 - val_accuracy: 0.9663\n",
      "Epoch 24/100\n",
      "21/21 [==============================] - 3s 148ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 0.9760\n",
      "Epoch 25/100\n",
      "21/21 [==============================] - 3s 154ms/step - loss: 0.0038 - accuracy: 0.9985 - val_loss: 4.2530e-04 - val_accuracy: 0.9808\n",
      "Epoch 26/100\n",
      "21/21 [==============================] - 3s 159ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.2558 - val_accuracy: 0.9856\n",
      "Epoch 27/100\n",
      "21/21 [==============================] - 3s 150ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0131 - val_accuracy: 0.9808\n",
      "Epoch 28/100\n",
      "21/21 [==============================] - 3s 149ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.2706e-04 - val_accuracy: 0.9856\n",
      "Epoch 29/100\n",
      "21/21 [==============================] - 3s 166ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0176 - val_accuracy: 0.9904\n",
      "Epoch 30/100\n",
      "21/21 [==============================] - 3s 141ms/step - loss: 0.0072 - accuracy: 0.9969 - val_loss: 0.1638 - val_accuracy: 0.9615\n",
      "Epoch 31/100\n",
      "21/21 [==============================] - 3s 144ms/step - loss: 0.0305 - accuracy: 0.9876 - val_loss: 0.0024 - val_accuracy: 0.9663\n",
      "Epoch 32/100\n",
      "21/21 [==============================] - 4s 169ms/step - loss: 0.0195 - accuracy: 0.9923 - val_loss: 0.0119 - val_accuracy: 0.9760\n",
      "Epoch 33/100\n",
      "21/21 [==============================] - 3s 147ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0065 - val_accuracy: 0.9760\n",
      "Epoch 34/100\n",
      "21/21 [==============================] - 3s 148ms/step - loss: 0.0052 - accuracy: 0.9985 - val_loss: 0.3659 - val_accuracy: 0.9760\n",
      "Epoch 35/100\n",
      "21/21 [==============================] - 3s 158ms/step - loss: 0.0036 - accuracy: 0.9985 - val_loss: 0.0790 - val_accuracy: 0.9808\n",
      "Epoch 36/100\n",
      "21/21 [==============================] - 3s 144ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 0.9808\n",
      "Epoch 37/100\n",
      "21/21 [==============================] - 3s 143ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0060 - val_accuracy: 0.9760\n",
      "Epoch 38/100\n",
      "21/21 [==============================] - 3s 154ms/step - loss: 0.0083 - accuracy: 0.9969 - val_loss: 0.0010 - val_accuracy: 0.9760\n",
      "Epoch 39/100\n",
      "21/21 [==============================] - 3s 151ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 0.9952\n",
      "Epoch 40/100\n",
      "21/21 [==============================] - 3s 146ms/step - loss: 0.0035 - accuracy: 0.9985 - val_loss: 0.0162 - val_accuracy: 0.9856\n",
      "Epoch 41/100\n",
      "21/21 [==============================] - 3s 149ms/step - loss: 0.0301 - accuracy: 0.9907 - val_loss: 0.0672 - val_accuracy: 0.9663\n",
      "Epoch 42/100\n",
      "21/21 [==============================] - 3s 158ms/step - loss: 0.0191 - accuracy: 0.9954 - val_loss: 0.0137 - val_accuracy: 0.9952\n",
      "Epoch 43/100\n",
      "21/21 [==============================] - 3s 157ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.1822 - val_accuracy: 0.9712\n",
      "Epoch 44/100\n",
      "21/21 [==============================] - 4s 179ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.2011 - val_accuracy: 0.9856\n",
      "Epoch 45/100\n",
      "21/21 [==============================] - 4s 169ms/step - loss: 0.0059 - accuracy: 0.9969 - val_loss: 0.0012 - val_accuracy: 0.9904\n",
      "Epoch 46/100\n",
      "21/21 [==============================] - 3s 147ms/step - loss: 0.0092 - accuracy: 0.9954 - val_loss: 0.8210 - val_accuracy: 0.9423\n",
      "Epoch 47/100\n",
      "21/21 [==============================] - 3s 154ms/step - loss: 0.0642 - accuracy: 0.9876 - val_loss: 0.4801 - val_accuracy: 0.8558\n",
      "Epoch 48/100\n",
      "21/21 [==============================] - 3s 143ms/step - loss: 0.1053 - accuracy: 0.9598 - val_loss: 0.0728 - val_accuracy: 0.9663\n",
      "Epoch 49/100\n",
      "21/21 [==============================] - 3s 155ms/step - loss: 0.0348 - accuracy: 0.9861 - val_loss: 0.0975 - val_accuracy: 0.9663\n",
      "Epoch 50/100\n",
      "21/21 [==============================] - 4s 188ms/step - loss: 0.0281 - accuracy: 0.9892 - val_loss: 0.0322 - val_accuracy: 0.9327\n",
      "Epoch 51/100\n",
      "21/21 [==============================] - 4s 175ms/step - loss: 0.0346 - accuracy: 0.9892 - val_loss: 0.1206 - val_accuracy: 0.9712\n",
      "Epoch 52/100\n",
      "21/21 [==============================] - 4s 168ms/step - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.2251 - val_accuracy: 0.9760\n",
      "Epoch 53/100\n",
      "21/21 [==============================] - 3s 145ms/step - loss: 0.0118 - accuracy: 0.9954 - val_loss: 0.0015 - val_accuracy: 0.9808\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amruth PC\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\utils\\data_utils.py:616: UserWarning: The input 9 could not be retrieved. It could be because a worker has died.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "## fitting model to images\n",
    "classifier.fit_generator(\n",
    "        train_set,\n",
    "        epochs=100,\n",
    "        validation_data=test_set\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 52ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.02534724585711956, 0.9807692170143127]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "               Pred        Img\n",
      "0       With Helmet    wh1.jpg\n",
      "1       With Helmet    wh2.jpg\n",
      "2       With Helmet    wh3.jpg\n",
      "3       With Helmet    wh4.jpg\n",
      "4       With Helmet    wh5.jpg\n",
      "5       With Helmet    wh7.jpg\n",
      "6       With Helmet   woh1.jpg\n",
      "7   With Out Helmet  woh10.jpg\n",
      "8       With Helmet  woh11.jpg\n",
      "9       With Helmet  woh12.jpg\n",
      "10  With Out Helmet   woh2.jpg\n",
      "11      With Helmet   woh3.jpg\n",
      "12      With Helmet   woh4.jpg\n",
      "13      With Helmet   woh5.jpg\n",
      "14      With Helmet   woh6.jpg\n",
      "15      With Helmet   woh7.jpg\n",
      "16      With Helmet   woh8.jpg\n",
      "17      With Helmet   woh9.jpg\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "names=[]\n",
    "pred=[]\n",
    "folder_path='C:/Users/Amruth PC/Desktop/SEM 2/AI/data/helmet_detection/single_prediction'\n",
    "for img in os.listdir(folder_path):\n",
    "    names.append(img)\n",
    "    img = os.path.join(folder_path, img)\n",
    "    img = image.load_img(img, target_size=(64, 64))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    images.append(img)\n",
    "\n",
    "# stack up images list to pass for prediction\n",
    "images = np.vstack(images)\n",
    "classes = classifier.predict_classes(images, batch_size=10)\n",
    "print(classes)\n",
    "for i in classes:\n",
    "    if(i==1):\n",
    "        pred.append('With Out Helmet')\n",
    "    else:\n",
    "        pred.append('With Helmet')\n",
    "data=pd.DataFrame(pred)\n",
    "data.columns=['Pred']\n",
    "data['Img']=names\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "## prediction of single new data\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image= image.load_img('C:/Users/Amruth PC/Desktop/SEM 2/AI/data/helmet_detection/single_prediction/abc.jpg'\n",
    "                           ,target_size =(128,128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64 at 0x21262DB8088>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert image to array\n",
    "test_image = image.img_to_array(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For single prediction change the dimension . \n",
    "\n",
    "test_image=test_image.reshape(1,128,128,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Class label of dog and cat\n",
    "\n",
    "if result == 1:\n",
    "    prediction = 'With Out Helmet'\n",
    "else:\n",
    "    prediction = 'With Helmet'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'With Helmet'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
